[
    {
        "client_msg_id": "3694edf0-7cff-48c1-b14d-cf5d4f5a2892",
        "type": "message",
        "text": "*Huge idea:* what if tensors are the next-generation replacement for RAM? Classic RAM (I'm talking about the software abstraction, not the physical hardware) is just a vector with 2^64 cells, most of which are zero and not backed by physical memory. This is commonly known as a _sparse_ vector. The current AI boom has made it obvious that higher-dimensional memory chunks, known as _tensors_, are an important idea, especially sparse ones. Other than being higher-dimensional, key differences between tensors and RAM include:\n\u2022 An AI app will typically work with multiple tensors, but a classical app will only work with one RAM. (Though Wasm can have multiple RAMs, known as \"linear memories\", and of course, you can pretend to have multiple memories using abstractions like _malloc_).\n\u2022 Tensors can be subjected to *unary operations* such as slicing, permuting, and aggregation (min, max, sum, product), that generalize the boring read and write operations on RAM.\n\u2022 Tensors can be subjected to *binary operations* such as multiplication/contraction (generalizing matrix multiplication), convolution, and element-wise addition.\nThe data of everyday programs is often very heterogeneous, which corresponds to having lots of *sparse* tensors. Sparse tensors need good support in software and _ideally_ in hardware. Thankfully, there is AI hardware being developed that is designed to operate on sparse tensors, by way of dedicated circuits that can compress and decompress them. <https://tenstorrent.com/|Tenstorrent> is probably the leader here.\n\nHere's a fun fact: multiplication of sparse Boolean tensors is equivalent to a database equi-join. So if you think databases are important, then maybe you should give tensors some thought.\n\nAnd relatedly: operations on tensors are typically massively-parallelizable, thus could be a good foundation for a high-performance programming language that compiles to AI hardware.",
        "user": "UCGAK10LS",
        "ts": "1630745854.074100",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UCGAK10LS",
            "ts": "1630745903.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "3dvvc",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Huge idea:",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " what if tensors are the next-generation replacement for RAM? Classic RAM (I'm talking about the software abstraction, not the physical hardware) is just a vector with 2^64 cells, most of which are zero and not backed by physical memory. This is commonly known as a "
                            },
                            {
                                "type": "text",
                                "text": "sparse",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " vector. The current AI boom has made it obvious that higher-dimensional memory chunks, known as "
                            },
                            {
                                "type": "text",
                                "text": "tensors",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": ", are an important idea, especially sparse ones. Other than being higher-dimensional, key differences between tensors and RAM include:\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_list",
                        "elements": [
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "An AI app will typically work with multiple tensors, but a classical app will only work with one RAM. (Though Wasm can have multiple RAMs, known as \"linear memories\", and of course, you can pretend to have multiple memories using abstractions like "
                                    },
                                    {
                                        "type": "text",
                                        "text": "malloc",
                                        "style": {
                                            "italic": true
                                        }
                                    },
                                    {
                                        "type": "text",
                                        "text": ")."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Tensors can be subjected to "
                                    },
                                    {
                                        "type": "text",
                                        "text": "unary operations",
                                        "style": {
                                            "bold": true
                                        }
                                    },
                                    {
                                        "type": "text",
                                        "text": " such as slicing, permuting, and aggregation (min, max, sum, product), that generalize the boring read and write operations on RAM."
                                    }
                                ]
                            },
                            {
                                "type": "rich_text_section",
                                "elements": [
                                    {
                                        "type": "text",
                                        "text": "Tensors can be subjected to "
                                    },
                                    {
                                        "type": "text",
                                        "text": "binary operations",
                                        "style": {
                                            "bold": true
                                        }
                                    },
                                    {
                                        "type": "text",
                                        "text": " such as multiplication/contraction (generalizing matrix multiplication), convolution, and element-wise addition."
                                    }
                                ]
                            }
                        ],
                        "style": "bullet",
                        "indent": 0,
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nThe data of everyday programs is often very heterogeneous, which corresponds to having lots of "
                            },
                            {
                                "type": "text",
                                "text": "sparse",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " tensors. Sparse tensors need good support in software and "
                            },
                            {
                                "type": "text",
                                "text": "ideally",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " in hardware. Thankfully, there is AI hardware being developed that is designed to operate on sparse tensors, by way of dedicated circuits that can compress and decompress them. "
                            },
                            {
                                "type": "link",
                                "url": "https://tenstorrent.com/",
                                "text": "Tenstorrent"
                            },
                            {
                                "type": "text",
                                "text": " is probably the leader here.\n\nHere's a fun fact: multiplication of sparse Boolean tensors is equivalent to a database equi-join. So if you think databases are important, then maybe you should give tensors some thought.\n\nAnd relatedly: operations on tensors are typically massively-parallelizable, thus could be a good foundation for a high-performance programming language that compiles to AI hardware."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "reply_count": 13,
        "reply_users_count": 5,
        "latest_reply": "1630867715.095800",
        "reply_users": [
            "UPVBV34EL",
            "UJZS8UUJV",
            "UCGAK10LS",
            "U01AD80KMLK",
            "UJBAJNFLK"
        ],
        "is_locked": false,
        "subscribed": false
    },
    {
        "client_msg_id": "36393e0b-fc8c-4a00-8f53-323f4e929253",
        "type": "message",
        "text": "&gt; And relatedly: operations on tensors are typically massively-parallelizable, thus could be a good foundation for a high-performance programming language that compiles to AI hardware. \nYou hooked me there",
        "user": "UPVBV34EL",
        "ts": "1630750683.074300",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "57nD",
                "elements": [
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "And relatedly: operations on tensors are typically massively-parallelizable, thus could be a good foundation for a high-performance programming language that compiles to AI hardware. "
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nYou hooked me there"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "24ecf77d-8b1a-424a-9909-7d9668338a70",
        "type": "message",
        "text": "So we already store tensors in RAM and perform various operations on them. You said software not hardware, so is the difference here that the abstraction between the 1D (flattened) data and its higher dimensional form is provided at a lower level in the software?",
        "user": "UJZS8UUJV",
        "ts": "1630778381.075000",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Aay",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "So we already store tensors in RAM and perform various operations on them. You said software not hardware, so is the difference here that the abstraction between the 1D (flattened) data and its higher dimensional form is provided at a lower level in the software?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "6EC69FC1-3F5F-42A3-B9A5-4D22C5C4E787",
        "type": "message",
        "text": "I mean the _assembly language_ of the hardware should be phrased in terms of operations on tensors. :slightly_smiling_face: The programmer should not be concerned with whether the tensor is ultimately flattened into a linear array of SRAM or DRAM cells. (In AI hardware, they definitely won\u2019t be.)",
        "user": "UCGAK10LS",
        "ts": "1630795361.078700",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Fn+Q",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I mean the "
                            },
                            {
                                "type": "text",
                                "text": "assembly language",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " of the hardware should be phrased in terms of operations on tensors. "
                            },
                            {
                                "type": "emoji",
                                "name": "slightly_smiling_face"
                            },
                            {
                                "type": "text",
                                "text": " The programmer should not be concerned with whether the tensor is ultimately flattened into a linear array of SRAM or DRAM cells. (In AI hardware, they definitely won\u2019t be.)"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UJZS8UUJV"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "CD966572-30CB-4B62-A574-B7DCB55748A0",
        "type": "message",
        "text": "My goal with this post is just to get people thinking a little differently about the memory model upon which a programming language is built. Tensor-based memory models are about to become mainstream (next 5 years) thanks to the AI boom. Could lead to some exciting new paradigms of programming.",
        "user": "UCGAK10LS",
        "ts": "1630795536.081700",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "8yc",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "My goal with this post is just to get people thinking a little differently about the memory model upon which a programming language is built. Tensor-based memory models are about to become mainstream (next 5 years) thanks to the AI boom. Could lead to some exciting new paradigms of programming."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "59167AAE-88EA-402C-8F60-79E2A09253BB",
        "type": "message",
        "text": "Here\u2019s a challenge for everyone: when you visualise the act of \u201callocating memory\u201d, what do you see? If you see a big linear chunk that you can address with a pointer, then maybe you\u2019re trapped in 1-dimensional thinking. I certainly was/am.",
        "user": "UCGAK10LS",
        "ts": "1630795876.084800",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "ErQrW",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Here\u2019s a challenge for everyone: when you visualise the act of \u201callocating memory\u201d, what do you see? If you see a big linear chunk that you can address with a pointer, then maybe you\u2019re trapped in 1-dimensional thinking. I certainly was/am."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "4bb1cabf-3823-4c83-bc87-e990f8fbce4c",
        "type": "message",
        "text": "How's your memory mostly zeros? If it is, you should use smaller machines. I think the idea that RAM is similar to a sparse vector is often not right. At least not if you use Chrome for browsing.",
        "user": "U01AD80KMLK",
        "ts": "1630806483.085100",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "/vz7",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "How's your memory mostly zeros? If it is, you should use smaller machines. I think the idea that RAM is similar to a sparse vector is often not right. At least not if you use Chrome for browsing."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "ABAF8820-147D-406E-9F72-DD64C3342F32",
        "type": "message",
        "text": "I\u2019m referring to virtual memory (i.e. what apps see). I guarantee you that your 64-bits of virtual memory are mostly zeroes! And with a paging system, you can write across vast swathes of the memory whilst only consuming physical resources for the pages you actually touch. That\u2019s what I mean when I describe linear memory as a sparse vector.",
        "user": "UCGAK10LS",
        "ts": "1630806818.088800",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UCGAK10LS",
            "ts": "1630806862.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "7He+A",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I\u2019m referring to virtual memory (i.e. what apps see). I guarantee you that your 64-bits of virtual memory are mostly zeroes! And with a paging system, you can write across vast swathes of the memory whilst only consuming physical resources for the pages you actually touch. That\u2019s what I mean when I describe linear memory as a sparse vector."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "118D103C-8CAD-40EA-9E74-FF2BE32E3373",
        "type": "message",
        "text": "Now imagine what it would be like to have a memory model where your memory is sparse at the _byte_-level, and is multidimensional, and you can have multiple memories and perform massively-parallel operations (such as aggregations) over them. This is what sparse tensors are.",
        "user": "UCGAK10LS",
        "ts": "1630807242.093300",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "aHN",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Now imagine what it would be like to have a memory model where your memory is sparse at the "
                            },
                            {
                                "type": "text",
                                "text": "byte",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": "-level, and is multidimensional, and you can have multiple memories and perform massively-parallel operations (such as aggregations) over them. This is what sparse tensors are."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "f35581f7-93a9-4004-b642-ee8eb6172d72",
        "type": "message",
        "text": "N-dimensional arrays as a fundamental data representation? That's an idea that has been around since the days of Fortran and APL. The 1960s. Efficient parallelization has been investigated as well, with today's Fortran containing very good support, though it's less automatic / miraculous than people tend to expect.",
        "user": "UJBAJNFLK",
        "ts": "1630829040.093600",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "nfi9",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "N-dimensional arrays as a fundamental data representation? That's an idea that has been around since the days of Fortran and APL. The 1960s. Efficient parallelization has been investigated as well, with today's Fortran containing very good support, though it's less automatic / miraculous than people tend to expect."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "e5db2dc7-b449-43b5-9c54-a7b03819e852",
        "type": "message",
        "text": "BTW, I avoid calling N-dimensional arrays tensors because a tensor for me is a algebraic and geometric object, not a data structure: <https://en.wikipedia.org/wiki/Tensor>",
        "user": "UJBAJNFLK",
        "ts": "1630829126.093800",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "DWSZ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "BTW, I avoid calling N-dimensional arrays tensors because a tensor for me is a algebraic and geometric object, not a data structure: "
                            },
                            {
                                "type": "link",
                                "url": "https://en.wikipedia.org/wiki/Tensor"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "b74a9a23-fc21-4f71-8d63-cd2887d30cc6",
        "type": "message",
        "text": "Does Fortran handle large and high-dimensional (10000x10000x10000x...) *sparse* tensors, though? That's the main enabler of a lot of interesting applications. Tenstorrent handles sparse tensors completely in hardware; as a programmer you work with them as if they were dense. For context: if you multiply a pair of 99% sparse tensors using a dense multiplication algorithm, you\u2019re doing 10000x more work than you need to (repeatedly multiplying by 0). In general, the asymptotic complexity is different.",
        "user": "UCGAK10LS",
        "ts": "1630831671.094000",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UCGAK10LS",
            "ts": "1630844066.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "WFJiJ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Does Fortran handle large and high-dimensional (10000x10000x10000x...) "
                            },
                            {
                                "type": "text",
                                "text": "sparse",
                                "style": {
                                    "bold": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " tensors, though? That's the main enabler of a lot of interesting applications. Tenstorrent handles sparse tensors completely in hardware; as a programmer you work with them as if they were dense. For context: if you multiply a pair of 99% sparse tensors using a dense multiplication algorithm, you\u2019re doing 10000x more work than you need to (repeatedly multiplying by 0). In general, the asymptotic complexity is different."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "0c2672be-3713-4e88-9405-49815004bd03",
        "type": "message",
        "text": "I'm aware of the more \"mathematical\" definition of tensor. But I believe the difference is just that the array representation is what you get once you've chosen a basis. You can also talk about tensors without reference to any basis.",
        "user": "UCGAK10LS",
        "ts": "1630831833.094200",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UCGAK10LS",
            "ts": "1630844097.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "sI0",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm aware of the more \"mathematical\" definition of tensor. But I believe the difference is just that the array representation is what you get once you've chosen a basis. You can also talk about tensors without reference to any basis."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    },
    {
        "client_msg_id": "5191dda9-5732-482c-9ac5-891ebdcd6dde",
        "type": "message",
        "text": "Fortran doesn't support sparse arrays as a language feature, but library support has been around for decades, getting better all the time.\n\nAs for the tensor, yes, once you pick a basis, you get an array representation. But the whole point of tensor algebra and tensor analysis is that the tensor has a meaning (and properties) _independent_ of the choice of a basis.",
        "user": "UJBAJNFLK",
        "ts": "1630867715.095800",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "pBdS",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Fortran doesn't support sparse arrays as a language feature, but library support has been around for decades, getting better all the time.\n\nAs for the tensor, yes, once you pick a basis, you get an array representation. But the whole point of tensor algebra and tensor analysis is that the tensor has a meaning (and properties) "
                            },
                            {
                                "type": "text",
                                "text": "independent",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " of the choice of a basis."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1630745854.074100",
        "parent_user_id": "UCGAK10LS"
    }
]