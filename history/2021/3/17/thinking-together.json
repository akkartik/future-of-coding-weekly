[
    {
        "client_msg_id": "8fa89ca1-bf9b-4e4d-b1de-28957eea5120",
        "type": "message",
        "text": "I had a shower thought about pushing to prod, wanted to share it with FoC.\n\nPushes to prod make no sense except for front end. A push to prod is basically FORCING EVERYBODY ONTO IT NOW. You used an old version? Too bad it's gone.\n\nFor front end, I get it, no human is going to visit <http://v1337.facebook.com|v1337.facebook.com>. But backend, where clients are programs? Makes no sense.\nWhy don't we use proper dependency management? Push a new version to prod. Let clients migrate manually to that new version.\nWant to break backwards compatibility, go for it. Push first (and use semver), fix clients later.\nIf clients want the latest and not have to upgrade manually, let them use a symbolic version \"latest.\" Just like the current system, only opt-in.\n\nWhy can't we have this world?",
        "user": "UL5AX4G2H",
        "ts": "1615997784.006600",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "igE9",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I had a shower thought about pushing to prod, wanted to share it with FoC.\n\nPushes to prod make no sense except for front end. A push to prod is basically FORCING EVERYBODY ONTO IT NOW. You used an old version? Too bad it's gone.\n\nFor front end, I get it, no human is going to visit "
                            },
                            {
                                "type": "link",
                                "url": "http://v1337.facebook.com",
                                "text": "v1337.facebook.com"
                            },
                            {
                                "type": "text",
                                "text": ". But backend, where clients are programs? Makes no sense.\nWhy don't we use proper dependency management? Push a new version to prod. Let clients migrate manually to that new version.\nWant to break backwards compatibility, go for it. Push first (and use semver), fix clients later.\nIf clients want the latest and not have to upgrade manually, let them use a symbolic version \"latest.\" Just like the current system, only opt-in.\n\nWhy can't we have this world?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "reply_count": 23,
        "reply_users_count": 7,
        "latest_reply": "1616006335.012700",
        "reply_users": [
            "UCUSW7WVD",
            "UBN9AFS0N",
            "UQ706GB9U",
            "UL5AX4G2H",
            "UDQBTJ211",
            "UC2A2ARPT",
            "U01RNN2M4KS"
        ],
        "subscribed": true,
        "last_read": "1616006335.012700",
        "reactions": [
            {
                "name": "heart",
                "users": [
                    "UCUSW7WVD"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "f7f191fe-a83e-4a0d-b9fe-4469bf8870d1",
        "type": "message",
        "text": "On the front-end side, this article has been thought-provoking to me for years: <https://www.ribbonfarm.com/2014/09/24/the-rhythms-of-information-flow-pacing-and-spacetime>",
        "user": "UCUSW7WVD",
        "ts": "1615998423.006800",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UCUSW7WVD",
            "ts": "1616000493.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "VMI",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "On the front-end side, this article has been thought-provoking to me for years: "
                            },
                            {
                                "type": "link",
                                "url": "https://www.ribbonfarm.com/2014/09/24/the-rhythms-of-information-flow-pacing-and-spacetime"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "cake",
                "users": [
                    "UC2A2ARPT",
                    "UA14TGLTC"
                ],
                "count": 2
            }
        ]
    },
    {
        "client_msg_id": "886aab88-aa83-4e03-9075-d90b62533631",
        "type": "message",
        "text": "because now instead of maintaining one version you are maintaining all releases you ever did, when there are problems or reports you don't know which versions they come from and it may generate a combinatoric explosion of reasons, you have to backport/forward port all fixes since reusing across versions may introduce issues if you refactor and introduce a bug (version 123 before a refactoring is no longer version 123)",
        "user": "UBN9AFS0N",
        "ts": "1615998951.007200",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "oD0",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "because now instead of maintaining one version you are maintaining all releases you ever did, when there are problems or reports you don't know which versions they come from and it may generate a combinatoric explosion of reasons, you have to backport/forward port all fixes since reusing across versions may introduce issues if you refactor and introduce a bug (version 123 before a refactoring is no longer version 123)"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "100",
                "users": [
                    "UDQKHNP51"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "392071d4-0580-4011-9271-740c1f0fe73a",
        "type": "message",
        "text": "REST supports version negotiation, nobody ever used it, we can barely maintain the current version",
        "user": "UBN9AFS0N",
        "ts": "1615998976.007400",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "u4CTL",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "REST supports version negotiation, nobody ever used it, we can barely maintain the current version"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "thinking_face",
                "users": [
                    "UA14TGLTC"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "7d01f565-0c34-4e9b-8085-298308e9f4e8",
        "type": "message",
        "text": "what do you do when dependencies break/bitrot/get security issues or stop being supported by the host? Performance improvements only work for users on the last version, have to keep support for all schemas and file formats in parallel. Monitoring is much harder to understand, you may get some performance issues when someone does something in some older version",
        "user": "UBN9AFS0N",
        "ts": "1615999151.007600",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "dRfT",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "what do you do when dependencies break/bitrot/get security issues or stop being supported by the host? Performance improvements only work for users on the last version, have to keep support for all schemas and file formats in parallel. Monitoring is much harder to understand, you may get some performance issues when someone does something in some older version"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "c44593cd-d62e-4d64-a6b5-b46885b3d086",
        "type": "message",
        "text": "I'm not saying it wouldn't be nice, just that we need to change almost everything we do and how we do it to support it.",
        "user": "UBN9AFS0N",
        "ts": "1615999171.007800",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Uu4OU",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I'm not saying it wouldn't be nice, just that we need to change almost everything we do and how we do it to support it."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UC2A2ARPT"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "f1c79a31-bc1c-46e6-b30b-d51a2e4d0b68",
        "type": "message",
        "text": "we did something like this in production with jboss modules in java. dynamically loading jars on request.",
        "user": "UQ706GB9U",
        "ts": "1615999237.008000",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cUg/Z",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "we did something like this in production with jboss modules in java. dynamically loading jars on request."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "2f990e78-e235-4a47-8e34-067417e03108",
        "type": "message",
        "text": "<@UBN9AFS0N> Re: maintenance, this is the same problem every library developer has and they publish versions, not \"latest\" only\n\nRe: performance, if we treat separate versions as separate services then we don't have conflation issues. If we want to combine stats, we can always combine them.",
        "user": "UL5AX4G2H",
        "ts": "1615999471.008300",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "YPi=S",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UBN9AFS0N"
                            },
                            {
                                "type": "text",
                                "text": " Re: maintenance, this is the same problem every library developer has and they publish versions, not \"latest\" only\n\nRe: performance, if we treat separate versions as separate services then we don't have conflation issues. If we want to combine stats, we can always combine them."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "a0982a7c-da9b-48b0-b79c-68477d8bdb2f",
        "type": "message",
        "text": "Cambria from ink&amp;switch follows through on this idea",
        "user": "UDQBTJ211",
        "ts": "1615999615.008500",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "cKgz",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Cambria from ink&switch follows through on this idea"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "dfbed86d-5d1d-4d30-aef2-c8c6ab4e15d9",
        "type": "message",
        "text": "to give you an idea of the scale of the one service has 700 active and 260K inactive versions collectively doing about 75K rps globally.",
        "user": "UQ706GB9U",
        "ts": "1615999815.008900",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "a7qE",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "to give you an idea of the scale of the one service has 700 active and 260K inactive versions collectively doing about 75K rps globally."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "exploding_head",
                "users": [
                    "UCUSW7WVD",
                    "UA14TGLTC"
                ],
                "count": 2
            }
        ]
    },
    {
        "client_msg_id": "6871f96d-9d0d-43c8-9176-0b53fd3d5756",
        "type": "message",
        "text": "Was it worth it in practice?",
        "user": "UL5AX4G2H",
        "ts": "1615999895.009200",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "07O",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Was it worth it in practice?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "ee365e2d-c93b-4257-bc77-b574e4f8ac3e",
        "type": "message",
        "text": "the system got too complex to run locally and meant developers couldn\u2019t debug locally anymore. We\u2019ve switched to a docker based system but it still cumbersome. humans are bad at cleaning up after themselves and we had to build lots of tools to track down underutilized (different than unused) to clean versions up as fast as they were making them.",
        "user": "UQ706GB9U",
        "ts": "1616000455.009600",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "6T3J",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "the system got too complex to run locally and meant developers couldn\u2019t debug locally anymore. We\u2019ve switched to a docker based system but it still cumbersome. humans are bad at cleaning up after themselves and we had to build lots of tools to track down underutilized (different than unused) to clean versions up as fast as they were making them."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "1faa3307-1d4c-4955-9297-598b77f12897",
        "type": "message",
        "text": "if you treat separate versions as separate services, which makes sense, now your server billing, maintenance, operations and monitoring costs/time increase with each new version. Also, not only support and issue tracking, documentation, tutorials, how tos, screenshots, videos have to consider all versions someone is using. I've maintained two major versions of the same product in production for a few customers and it's not fun.",
        "user": "UBN9AFS0N",
        "ts": "1616001430.010000",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "qBZRZ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "if you treat separate versions as separate services, which makes sense, now your server billing, maintenance, operations and monitoring costs/time increase with each new version. Also, not only support and issue tracking, documentation, tutorials, how tos, screenshots, videos have to consider all versions someone is using. I've maintained two major versions of the same product in production for a few customers and it's not fun."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "5075f4f7-2b13-41c4-9b01-8978839f4f59",
        "type": "message",
        "text": "I've had many cases where you get an issue in a dependency, report it and the maintainer (with good reasons) tells you to upgrade to latest major since the version you are reporting is no longer maintained",
        "user": "UBN9AFS0N",
        "ts": "1616001636.010200",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "kWK2n",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I've had many cases where you get an issue in a dependency, report it and the maintainer (with good reasons) tells you to upgrade to latest major since the version you are reporting is no longer maintained"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "2ed7c75d-cdd8-475f-af09-c71aa2ea6d42",
        "type": "message",
        "text": "Billing/maintenance/ops/monitoring is different from standard dependencies. I suspect these are long term automateable problems (i.e. serverless), but they are real today. Everything else sounds the same dilemma as standard dependencies though?\n\nThe story of reporting an issue in a dependency and being told that version is deprecated and to upgrade sounds good to me. It's better than being forceably upgraded and the old one disappearing.",
        "user": "UL5AX4G2H",
        "ts": "1616002419.010400",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "tFn",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Billing/maintenance/ops/monitoring is different from standard dependencies. I suspect these are long term automateable problems (i.e. serverless), but they are real today. Everything else sounds the same dilemma as standard dependencies though?\n\nThe story of reporting an issue in a dependency and being told that version is deprecated and to upgrade sounds good to me. It's better than being forceably upgraded and the old one disappearing."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "13540a60-53d1-49d2-8bd9-9a955f06811e",
        "type": "message",
        "text": "<@UQ706GB9U> Too complex to run locally because there were too many services for a computer to run? It's a fair point that running an http server is more involved than installing a package to disk",
        "user": "UL5AX4G2H",
        "ts": "1616002549.010600",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "q0J+E",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UQ706GB9U"
                            },
                            {
                                "type": "text",
                                "text": " Too complex to run locally because there were too many services for a computer to run? It's a fair point that running an http server is more involved than installing a package to disk"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "ed29d182-8040-4b0e-ba14-929a3d90ffe8",
        "type": "message",
        "text": "Datomic is probably a great analog. When used as intended, you never do away with your old data. All data is stored with an explicit sense of time. (<@UCUSW7WVD>'s linked article was good!) You can have many servers reading the database, and the each have a temporally-locked view of the data. Your data will never unexpectedly change out from under you.\n\nDatomic works great when you've designed a whole system around the way that it works. It's not a drop-in replacement for Postgres or Mongo.\n\nHere's another example \u2014 Basecamp (and Highrise) have their \"until the end of the internet\" practice, where users of old versions of their products will not be forced to upgrade. My company is still using the original Basecamp, which is 2 or 3 major reinventions old at this point. It still works great for our needs, in ways that the newer products wouldn't.\n\nIt's totally possible, and even practical, to keep old things alive when new things come into existence. You just need to design with that goal in mind, and that probably will demand confronting and rebuking some established practices and tacit assumptions. (In other words \u2014\u00a0what Mariano said, just with less _here are all the things that would be different_ and more _here are places where we already do this and it's fine/good.)_",
        "user": "UC2A2ARPT",
        "ts": "1616002795.010800",
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UC2A2ARPT",
            "ts": "1616003061.000000"
        },
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "B/1",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Datomic is probably a great analog. When used as intended, you never do away with your old data. All data is stored with an explicit sense of time. ("
                            },
                            {
                                "type": "user",
                                "user_id": "UCUSW7WVD"
                            },
                            {
                                "type": "text",
                                "text": "'s linked article was good!) You can have many servers reading the database, and the each have a temporally-locked view of the data. Your data will never unexpectedly change out from under you.\n\nDatomic works great when you've designed a whole system around the way that it works. It's not a drop-in replacement for Postgres or Mongo.\n\nHere's another example \u2014 Basecamp (and Highrise) have their \"until the end of the internet\" practice, where users of old versions of their products will not be forced to upgrade. My company is still using the original Basecamp, which is 2 or 3 major reinventions old at this point. It still works great for our needs, in ways that the newer products wouldn't.\n\nIt's totally possible, and even practical, to keep old things alive when new things come into existence. You just need to design with that goal in mind, and that probably will demand confronting and rebuking some established practices and tacit assumptions. (In other words \u2014\u00a0what Mariano said, just with less "
                            },
                            {
                                "type": "text",
                                "text": "here are all the things that would be different",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " and more "
                            },
                            {
                                "type": "text",
                                "text": "here are places where we already do this and it's fine/good.)",
                                "style": {
                                    "italic": true
                                }
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "416de89b-77bd-42f6-9d38-935159e4d45c",
        "type": "message",
        "text": "maintaining a small number of major versions is doable, maintaining all versions or at least say 10 versions is another thing",
        "user": "UBN9AFS0N",
        "ts": "1616005559.011500",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "+rf",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "maintaining a small number of major versions is doable, maintaining all versions or at least say 10 versions is another thing"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "739e79a0-0315-4c95-a663-f93a3aa4b763",
        "type": "message",
        "text": "maintaning some major versions is almost the same as maintaining a family of products, easier if you don't have to keep backward/forward compatibility, a little harder if you have to, but still doable.",
        "user": "UBN9AFS0N",
        "ts": "1616005647.011700",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "96i",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "maintaning some major versions is almost the same as maintaining a family of products, easier if you don't have to keep backward/forward compatibility, a little harder if you have to, but still doable."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "a49dcf09-3e83-45f0-96a6-7c65105d0d46",
        "type": "message",
        "text": "each version adds extra maintenance overhead, if it gets \"frozen\" the overhead may be small, but each extra version adds extra overhead on top, so you have to see if it makes sense.",
        "user": "UBN9AFS0N",
        "ts": "1616005774.011900",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "GjQZ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "each version adds extra maintenance overhead, if it gets \"frozen\" the overhead may be small, but each extra version adds extra overhead on top, so you have to see if it makes sense."
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "d64da5c8-18f3-4760-a717-da9a8a39824f",
        "type": "message",
        "text": "guix, containers, cambria and unison may make it easier",
        "user": "UBN9AFS0N",
        "ts": "1616005853.012100",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "9Jtf",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "guix, containers, cambria and unison may make it easier"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "25f34a96-ed17-4edd-9fad-3bffbd25a9a9",
        "type": "message",
        "text": "just an example, the weather widget on my OS stopped showing weather forecast for the following days, it seems the external api broke/changed,  they didn't change anything, yet it broke",
        "user": "UBN9AFS0N",
        "ts": "1616005919.012300",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "uCKX",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "just an example, the weather widget on my OS stopped showing weather forecast for the following days, it seems the external api broke/changed,  they didn't change anything, yet it broke"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "d3ec1b78-0db1-46b0-8f9f-688c5ebe7618",
        "type": "message",
        "text": "so it's not only your code, dependencies and environment, but all external systems too",
        "user": "UBN9AFS0N",
        "ts": "1616005965.012500",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "RPEf",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "so it's not only your code, dependencies and environment, but all external systems too"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H"
    },
    {
        "client_msg_id": "935bdb92-1dbd-4fa7-97ea-3af7f3d96842",
        "type": "message",
        "text": "TLDR: to me this is a pretty simple cost-benefit tradeoff. The reduction in regressions/confused users is rarely worth the cost (imo).\n\n(Others have covered some of the points I make here as I wrote this, so apologies if I\u2019m repeating anything)\n\nFor context, I cut my teeth in the bad old days of on-premise enterprise software. Our software was mission-critical, and it was incredibly difficult to entice customers to upgrade to new versions. For any given customer, the upgrade process could take a year or longer to work itself out. We probably had at least a half-dozen officially supported versions, and a handful of customer-specific releases on top. The decision-making process for determining what changes went where was a nightmare, to say nothing of the actual implementation. Our release manager was super-human. I never want to go back to that world again :)\n\nVersion compatibility is a pretty well understood problem, but supporting multiple versions simultaneously carries some pretty significant engineering and operational overhead. Running multiple versions of application logic is pretty straightforward, but there\u2019s added complexity in request routing.\n\nTechnically, it _shouldn\u2019t_ require more compute capacity to support multiple versions of a service, since each user can presumably only use one version at a time, but in practice redundancy means you\u2019re going to need more infrastructure to support multiple versions for the same user base. So that adds more cost.\n\nEach live version of the code adds operational complexity of monitoring and diagnosing issues. Two or three versions might be manageable. Ten? Impossible (imo). Each live version increases complexity linearly if not exponentially. Continuous deployment means you\u2019re pushing dozens of versions each week. There\u2019s no way a team could stand up separate instances of each of these and maintain their sanity. Although the canary process does resemble this motion, it\u2019s only managing two versions at a time, and for a limited time frame.\n\nThe temptation then would be to only stand up new instances for \u201cbreaking\u201d changes. But it can be devilishly difficult to decide what a breaking change even is. For example, consider an enumerated data type. Simply adding a value to an enum is technically a breaking change, because existing clients can\u2019t be guaranteed to know what to do with it. And that\u2019s just on the API interface side. At least there you can do static analysis to detect breaking changes, although the tooling needed to do this adds its own overhead. Many breaking changes manifest in behavior or semantics, which are impossible to detect statically, and can be very difficult to detect with testing or human reasoning. So, the decision of whether a given release warrants a separate instance becomes a risk management exercise, which can be costly in its own right.\n\nPlanning gets more painful too with each live version. Security or critical bug discovered? Get ready for a lot of painful overhead and difficult conversations deciding to which versions the fix should be backported. Depending on how much drift there is in the code from one version to the next, it\u2019s often not even obvious _how_ to backport a fix.\n\nFinally, these challenges often come to a head at the data layer. What happens when a new feature or bugfix requires a data migration? Unless you\u2019re willing to maintain separate databases as well, it\u2019s often practically impossible to support multiple versions, and even when it is, it adds yet more developer overhead.\n\nThere are solutions to all of these problems, but in my experience (over 20 years, for what that\u2019s worth), it has never been worth the effort. While not exactly easy, it\u2019s generally much more practical to adopt a posture of runtime compatibility with all live clients, with a multi-phase rollout process in the rare case where a breaking change is unavoidable. If you have a good client upgrade pipeline (browser-based client or app-store) this is pretty manageable.\n\nNote: <@UC2A2ARPT> makes a great point re: Datomic/versioned data. Everything I said is based on the typical \u201cmutation-friendly\u201d architecture. It is probably possible to architect a service to accommodate many live versions practically, but this would have to be a fundamental design goal from the beginning. I\u2019m not convinced there\u2019s away to completely avoid at least some of the costs, however, so imo there\u2019d better be a pretty good business reason to adopt this goal.\n\nAs we move forward and more of the software-using public becomes accustomed to and expects their software to keep improving, I\u2019m not convinced there will ever be more than a sliver of the user base that wants to stay on older versions. The question is: how much is supporting this minority of users worth to you?",
        "user": "U01RNN2M4KS",
        "ts": "1616006335.012700",
        "team": "T5TCAFTA9",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "65lT0",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "TLDR: to me this is a pretty simple cost-benefit tradeoff. The reduction in regressions/confused users is rarely worth the cost (imo).\n\n(Others have covered some of the points I make here as I wrote this, so apologies if I\u2019m repeating anything)\n\nFor context, I cut my teeth in the bad old days of on-premise enterprise software. Our software was mission-critical, and it was incredibly difficult to entice customers to upgrade to new versions. For any given customer, the upgrade process could take a year or longer to work itself out. We probably had at least a half-dozen officially supported versions, and a handful of customer-specific releases on top. The decision-making process for determining what changes went where was a nightmare, to say nothing of the actual implementation. Our release manager was super-human. I never want to go back to that world again :)\n\nVersion compatibility is a pretty well understood problem, but supporting multiple versions simultaneously carries some pretty significant engineering and operational overhead. Running multiple versions of application logic is pretty straightforward, but there\u2019s added complexity in request routing.\n\nTechnically, it "
                            },
                            {
                                "type": "text",
                                "text": "shouldn\u2019t",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " require more compute capacity to support multiple versions of a service, since each user can presumably only use one version at a time, but in practice redundancy means you\u2019re going to need more infrastructure to support multiple versions for the same user base. So that adds more cost.\n\nEach live version of the code adds operational complexity of monitoring and diagnosing issues. Two or three versions might be manageable. Ten? Impossible (imo). Each live version increases complexity linearly if not exponentially. Continuous deployment means you\u2019re pushing dozens of versions each week. There\u2019s no way a team could stand up separate instances of each of these and maintain their sanity. Although the canary process does resemble this motion, it\u2019s only managing two versions at a time, and for a limited time frame.\n\nThe temptation then would be to only stand up new instances for \u201cbreaking\u201d changes. But it can be devilishly difficult to decide what a breaking change even is. For example, consider an enumerated data type. Simply adding a value to an enum is technically a breaking change, because existing clients can\u2019t be guaranteed to know what to do with it. And that\u2019s just on the API interface side. At least there you can do static analysis to detect breaking changes, although the tooling needed to do this adds its own overhead. Many breaking changes manifest in behavior or semantics, which are impossible to detect statically, and can be very difficult to detect with testing or human reasoning. So, the decision of whether a given release warrants a separate instance becomes a risk management exercise, which can be costly in its own right.\n\nPlanning gets more painful too with each live version. Security or critical bug discovered? Get ready for a lot of painful overhead and difficult conversations deciding to which versions the fix should be backported. Depending on how much drift there is in the code from one version to the next, it\u2019s often not even obvious "
                            },
                            {
                                "type": "text",
                                "text": "how",
                                "style": {
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " to backport a fix.\n\nFinally, these challenges often come to a head at the data layer. What happens when a new feature or bugfix requires a data migration? Unless you\u2019re willing to maintain separate databases as well, it\u2019s often practically impossible to support multiple versions, and even when it is, it adds yet more developer overhead.\n\nThere are solutions to all of these problems, but in my experience (over 20 years, for what that\u2019s worth), it has never been worth the effort. While not exactly easy, it\u2019s generally much more practical to adopt a posture of runtime compatibility with all live clients, with a multi-phase rollout process in the rare case where a breaking change is unavoidable. If you have a good client upgrade pipeline (browser-based client or app-store) this is pretty manageable.\n\nNote: "
                            },
                            {
                                "type": "user",
                                "user_id": "UC2A2ARPT"
                            },
                            {
                                "type": "text",
                                "text": " makes a great point re: Datomic/versioned data. Everything I said is based on the typical \u201cmutation-friendly\u201d architecture. It is probably possible to architect a service to accommodate many live versions practically, but this would have to be a fundamental design goal from the beginning. I\u2019m not convinced there\u2019s away to completely avoid at least some of the costs, however, so imo there\u2019d better be a pretty good business reason to adopt this goal.\n\nAs we move forward and more of the software-using public becomes accustomed to and expects their software to keep improving, I\u2019m not convinced there will ever be more than a sliver of the user base that wants to stay on older versions. The question is: how much is supporting this minority of users worth to you?"
                            }
                        ]
                    }
                ]
            }
        ],
        "thread_ts": "1615997784.006600",
        "parent_user_id": "UL5AX4G2H",
        "reactions": [
            {
                "name": "100",
                "users": [
                    "U8A5MS6R1"
                ],
                "count": 1
            }
        ]
    }
]