[
    {
        "client_msg_id": "D8D542B1-E387-4176-8019-FB86B21A41DD",
        "type": "message",
        "text": "<@UK3LH8CF5> what do you make of section 4 of <https://www.noemamag.com/gpts-very-inhuman-mind/|https://www.noemamag.com/gpts-very-inhuman-mind/>",
        "user": "U6KQ2S410",
        "ts": "1680218483.279849",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "K1i/Y",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "user",
                                "user_id": "UK3LH8CF5"
                            },
                            {
                                "type": "text",
                                "text": " what do you make of section 4 of "
                            },
                            {
                                "type": "link",
                                "url": "https://www.noemamag.com/gpts-very-inhuman-mind/",
                                "text": "https://www.noemamag.com/gpts-very-inhuman-mind/"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "attachments": [
            {
                "from_url": "https://www.noemamag.com/gpts-very-inhuman-mind/",
                "image_url": "https://noemamag.imgix.net/2023/03/1000x2000.jpg?fit=crop&fm=pjpg&h=628&ixlib=php-3.3.0&w=1200&wpsize=noema-social-facebook&s=0988e35301a10dc859c150605407c8d0",
                "image_width": 1200,
                "image_height": 628,
                "image_bytes": 116951,
                "service_icon": "https://www.noemamag.com/wp-content/uploads/2020/06/cropped-ms-icon-310x310-1-180x180.png",
                "id": 1,
                "original_url": "https://www.noemamag.com/gpts-very-inhuman-mind/",
                "fallback": "NOEMA: GPT\u2019s Very Inhuman Mind | NOEMA",
                "text": "Like Narcissus and his reflection, too many AI researchers are searching for the essence of intelligence when it doesn\u2019t exist.",
                "title": "GPT\u2019s Very Inhuman Mind | NOEMA",
                "title_link": "https://www.noemamag.com/gpts-very-inhuman-mind/",
                "service_name": "NOEMA"
            }
        ],
        "thread_ts": "1680218483.279849",
        "reply_count": 6,
        "reply_users_count": 5,
        "latest_reply": "1680511879.641099",
        "reply_users": [
            "UAJKEBGP8",
            "UE6EFEPTQ",
            "UK3LH8CF5",
            "UBKNXPBAB",
            "UA14TGLTC"
        ],
        "is_locked": false,
        "subscribed": false
    },
    {
        "client_msg_id": "547DF786-C347-499C-8C08-139C7E2AEE23",
        "type": "message",
        "text": "Invokes Ryle and Dennett",
        "user": "U6KQ2S410",
        "ts": "1680218524.340119",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "NUJ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Invokes Ryle and Dennett"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9"
    },
    {
        "client_msg_id": "af75a998-2603-4f1d-b8af-fa0566f26f7b",
        "type": "message",
        "text": "emphasis on \u201cArtificial\u201d from \u201cArtificial Intelligence\u201d",
        "user": "UAJKEBGP8",
        "ts": "1680224664.638899",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "S0Qj",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "emphasis on \u201cArtificial\u201d from \u201cArtificial Intelligence\u201d"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410"
    },
    {
        "client_msg_id": "8442fa1c-d1bd-4180-887a-9751074e959a",
        "type": "message",
        "text": "Example of using LLM and classical verifier (a compiler for some DSL) to boost an LLM's code generation:\n\n<https://arxiv.org/pdf/2303.14100.pdf>\n\n<https://www.youtube.com/watch?v=-87yrXytluw>",
        "user": "U01M3Q6JEG7",
        "ts": "1680226045.316259",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "bHA",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Example of using LLM and classical verifier (a compiler for some DSL) to boost an LLM's code generation:\n\n"
                            },
                            {
                                "type": "link",
                                "url": "https://arxiv.org/pdf/2303.14100.pdf"
                            },
                            {
                                "type": "text",
                                "text": "\n\n"
                            },
                            {
                                "type": "link",
                                "url": "https://www.youtube.com/watch?v=-87yrXytluw"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "attachments": [
            {
                "from_url": "https://www.youtube.com/watch?v=-87yrXytluw",
                "service_icon": "https://a.slack-edge.com/80588/img/unfurl_icons/youtube.png",
                "thumb_url": "https://i.ytimg.com/vi/-87yrXytluw/hqdefault.jpg",
                "thumb_width": 480,
                "thumb_height": 360,
                "video_html": "<iframe width=\"400\" height=\"225\" src=\"https://www.youtube.com/embed/-87yrXytluw?feature=oembed&autoplay=1&iv_load_policy=3\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen title=\"CLAIRify: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting\"></iframe>",
                "video_html_width": 400,
                "video_html_height": 225,
                "id": 1,
                "original_url": "https://www.youtube.com/watch?v=-87yrXytluw",
                "fallback": "YouTube Video: CLAIRify: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting",
                "title": "CLAIRify: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting",
                "title_link": "https://www.youtube.com/watch?v=-87yrXytluw",
                "author_name": "RA^2D: Robotics-assisted Accelerated Discovery",
                "author_link": "https://www.youtube.com/@ra2drobotics-assistedaccel889",
                "service_name": "YouTube",
                "service_url": "https://www.youtube.com/"
            }
        ],
        "reactions": [
            {
                "name": "+1",
                "users": [
                    "UA14TGLTC"
                ],
                "count": 1
            }
        ]
    },
    {
        "client_msg_id": "c00263a4-9b21-4f8e-8d2b-4aac33ad30ad",
        "type": "message",
        "text": "Can't believe the internet is still arguing over whether ChatGPT is just \"glorified autocomplete\" :face_with_rolling_eyes:",
        "user": "UE6EFEPTQ",
        "ts": "1680260866.732939",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Z=9d8",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Can't believe the internet is still arguing over whether ChatGPT is just \"glorified autocomplete\" "
                            },
                            {
                                "type": "emoji",
                                "name": "face_with_rolling_eyes",
                                "unicode": "1f644"
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410"
    },
    {
        "client_msg_id": "f1169dcf-a2a6-47ce-befe-b2b47c10eaa8",
        "type": "message",
        "text": "It's a well written article. But there isn't much of an argument there. Can you define knowledge, understanding, intention, etc behavioristically? Well the article seems to assert you can, but why think that is the case?\n\nThe evidence we seem to be given in the article is two fold 1) Look at ChatGPT doing all these things, how can you deny it understands? 2) We can take the intentional stance towards the system and it works remarkably well.\n\nI'm going to assume the article doesn't believe its presenting an argument, because if so, its a rather lackluster one. Philosophers have already talked about systems like ChatGPT well before they existed. Searle's Chinese room argument asks the exact question being raised, can a computer program be behavioristically identical to a human while not understanding?\n\nVarious people fall on various sides of this argument. But I don't really think ChatGPT has really any bearing on it. Of course that doesn't stop philosophers like David Chalmers from embarrassing themselves by making silly statements.",
        "user": "UK3LH8CF5",
        "ts": "1680282970.147169",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "c0Ep",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "It's a well written article. But there isn't much of an argument there. Can you define knowledge, understanding, intention, etc behavioristically? Well the article seems to assert you can, but why think that is the case?\n\nThe evidence we seem to be given in the article is two fold 1) Look at ChatGPT doing all these things, how can you deny it understands? 2) We can take the intentional stance towards the system and it works remarkably well.\n\nI'm going to assume the article doesn't believe its presenting an argument, because if so, its a rather lackluster one. Philosophers have already talked about systems like ChatGPT well before they existed. Searle's Chinese room argument asks the exact question being raised, can a computer program be behavioristically identical to a human while not understanding?\n\nVarious people fall on various sides of this argument. But I don't really think ChatGPT has really any bearing on it. Of course that doesn't stop philosophers like David Chalmers from embarrassing themselves by making silly statements."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410",
        "reactions": [
            {
                "name": "heavy_check_mark",
                "users": [
                    "U6KQ2S410",
                    "UA14TGLTC"
                ],
                "count": 2
            }
        ]
    },
    {
        "client_msg_id": "49d6fb5b-5aff-43cf-beec-ba97015adda3",
        "type": "message",
        "text": "I found the article (or section 4 at least) quite useful. Not because it made an argument about what understanding etc. are in a philosophical sense (I sort of care about that stuff, but not really?), but because it helped me make sense of deflationary sleights of hand in the discourse.\n\nWhen Bender et al. call GPT a \u201cstochastic parrot\u201d or Chiang calls it a \u201cblurry JPEG of the web\u201d, they are _*not*_ making a Chinese-room argument about what understanding really is. They are making an argument about what GPT is _*capable of*_, based on its make-up, an argument which seems to be demonstrably flawed. The \u201cdeepity\u201d concept helped me tidy up this dynamic. As far as I\u2019m concerned, the rest of the article may be making a misstep by making it sound like this is about deep questions of philosophy.",
        "user": "UBKNXPBAB",
        "ts": "1680315015.475349",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "/uJ",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I found the article (or section 4 at least) quite useful. Not because it made an argument about what understanding etc. are in a philosophical sense (I sort of care about that stuff, but not really?), but because it helped me make sense of deflationary sleights of hand in the discourse.\n\nWhen Bender et al. call GPT a \u201cstochastic parrot\u201d or Chiang calls it a \u201cblurry JPEG of the web\u201d, they are "
                            },
                            {
                                "type": "text",
                                "text": "not",
                                "style": {
                                    "bold": true,
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": " making a Chinese-room argument about what understanding really is. They are making an argument about what GPT is "
                            },
                            {
                                "type": "text",
                                "text": "capable of",
                                "style": {
                                    "bold": true,
                                    "italic": true
                                }
                            },
                            {
                                "type": "text",
                                "text": ", based on its make-up, an argument which seems to be demonstrably flawed. The \u201cdeepity\u201d concept helped me tidy up this dynamic. As far as I\u2019m concerned, the rest of the article may be making a misstep by making it sound like this is about deep questions of philosophy."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410"
    },
    {
        "client_msg_id": "19b2238b-2c92-40cc-95be-7166b9c28a87",
        "type": "message",
        "text": "I need to spend more time reading the Stochastic Parrot paper, but from my reading they are not at questioning the output of GPT style models. They seem to think they are very capable of outputting fluent speech but lack things necessary for understanding.\n\n&gt; Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model).\nI'm not saying I necessarily agree with everything here. Just that this does seem to be the argument they are making. The deepity point was a bit off imo. From the article:\n\n&gt; The other meaning is the one that suggests that ChatGPT has no understanding of communicative intent, so when you ask it a question, it can only respond correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers. But in this sense, ChatGPT is obviously not a stochastic parrot. You can ask it all sorts of subtle things, questions it has never seen before and which cannot be answered without understanding.\nThen there are some transcriptions demonstrating understanding of \"communicative intent\". But that's to misunderstand the point the paper was making, not to refute it. Of course GPT can talk about communicative intent. The point being made in the paper is that the ability to talk about communicative intent doesn't entail that it has communicative intent or pays attention to our communicative intent.",
        "user": "UK3LH8CF5",
        "ts": "1680316282.013809",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "Msyz6",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "I need to spend more time reading the Stochastic Parrot paper, but from my reading they are not at questioning the output of GPT style models. They seem to think they are very capable of outputting fluent speech but lack things necessary for understanding.\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model)."
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nI'm not saying I necessarily agree with everything here. Just that this does seem to be the argument they are making. The deepity point was a bit off imo. From the article:\n\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "The other meaning is the one that suggests that ChatGPT has no understanding of communicative intent, so when you ask it a question, it can only respond correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers. But in this sense, ChatGPT is obviously not a stochastic parrot. You can ask it all sorts of subtle things, questions it has never seen before and which cannot be answered without understanding."
                            }
                        ],
                        "border": 0
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "\nThen there are some transcriptions demonstrating understanding of \"communicative intent\". But that's to misunderstand the point the paper was making, not to refute it. Of course GPT can talk about communicative intent. The point being made in the paper is that the ability to talk about communicative intent doesn't entail that it has communicative intent or pays attention to our communicative intent."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "edited": {
            "user": "UK3LH8CF5",
            "ts": "1680321133.000000"
        },
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410"
    },
    {
        "client_msg_id": "775b1d0c-fbf1-46ce-af3f-d6d604351214",
        "type": "message",
        "text": "Hmm...\n&gt; [Text generated by an LM] can't have been [grounded in communicative intent], because the training data never included sharing thoughts with a listener\nDoes this ring at all true to you guys?  I think of the \"the training data\" as being things people have written online, which almost always involves \"sharing thoughts with a listener.\"\n\nI take the fact that ChatGPT gives sensible answers in a way that previous GPTs did not as evidence that the model can at least preserve intent and perhaps recombine so as to generate new intent.",
        "user": "UA14TGLTC",
        "ts": "1680511879.641099",
        "blocks": [
            {
                "type": "rich_text",
                "block_id": "lHH",
                "elements": [
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Hmm...\n"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_quote",
                        "elements": [
                            {
                                "type": "text",
                                "text": "[Text generated by an LM] can't have been [grounded in communicative intent], because the training data never included sharing thoughts with a listener"
                            }
                        ]
                    },
                    {
                        "type": "rich_text_section",
                        "elements": [
                            {
                                "type": "text",
                                "text": "Does this ring at all true to you guys?  I think of the \"the training data\" as being things people have written online, which almost always involves \"sharing thoughts with a listener.\"\n\nI take the fact that ChatGPT gives sensible answers in a way that previous GPTs did not as evidence that the model can at least preserve intent and perhaps recombine so as to generate new intent."
                            }
                        ]
                    }
                ]
            }
        ],
        "team": "T5TCAFTA9",
        "thread_ts": "1680218483.279849",
        "parent_user_id": "U6KQ2S410"
    }
]