
<!doctype html>
<html>
  <head>
    <meta charset=utf-8>
    <title>Future of Coding History</title>
    <link rel="stylesheet" href="https://marianoguerra.github.io/future-of-coding-weekly/history/style.css">
  </head>
  <body>
    <div id="ui">
      <a id="logo" href="https://futureofcoding.org">
        <img src="https://marianoguerra.github.io/future-of-coding-weekly/history/logo.svg" alt="Future of Coding History">
      </a>
      <div id="small-logo">
        <a href="https://futureofcoding.org">Future of Coding</a> History
      </div>
      <div id="center">
        <h4>
          You are viewing archived messages.<br>
          Go <a href="https://marianoguerra.github.io/future-of-coding-weekly/history">here</a> to search the history.
        </h4>
      </div>
      <div id="actions"></div>
    </div>
    <div id="msgs-output">
<div id="2023-03-28T22:22:30.081Z" class="post"><span class="user">Paul Tarvydas</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:22:30.081Z" class="date">2023-03-28 22:22:30</a> <div class="message"><p><strong>Paul Tarvydas</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:24:34.099Z" class="post"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:24:34.099Z" class="date">2023-03-28 22:24:34</a> <div class="message"><p><strong>Jonathan Edwards</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:25:13.391Z" class="post"><span class="user">Jude Rayan</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:25:13.391Z" class="date">2023-03-28 22:25:13</a> <div class="message"><p><strong>Jude Rayan</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:32:06.643Z" class="post"><span class="user">Nick Smith</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:32:06.643Z" class="date">2023-03-28 22:32:06</a> <div class="message"><p><strong>Nick Smith</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:33:37.161Z" class="post"><span class="user">Duncan Cragg</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:33:37.161Z" class="date">2023-03-28 22:33:37</a> <div class="message"><p><strong>Duncan Cragg</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:38:39.247Z" class="post"><span class="user">Harrison Shoebridge</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:38:39.247Z" class="date">2023-03-28 22:38:39</a> <div class="message"><p><strong>Harrison Shoebridge</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:39:29.841Z" class="post"><span class="user">Robin Allison</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:39:29.841Z" class="date">2023-03-28 22:39:29</a> <div class="message"><p><strong>Robin Allison</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:40:07.098Z" class="post"><span class="user">Duncan Cragg</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:40:07.098Z" class="date">2023-03-28 22:40:07</a> <div class="message"><p>Some links to threads I pulled together,  won't be complete of course:</p>
<p><a href="./?fromDate=2023-03-23&amp;toDate=2023-03-29&amp;channel=of-end-user-programming&amp;filter=#2023-03-26T12:24:29.621Z">💬 #of-end-user-programming@2023-03-26T12:24:29.621Z</a></p>
<p><a href="https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679642239661619">futureofcoding.slack.com/archives/C5T9GPWFL/p1679642239661619</a></p>
<p><a href="./?fromDate=2023-03-24&amp;toDate=2023-03-30&amp;channel=thinking-together&amp;filter=#2023-03-27T07:30:27.016Z">💬 #thinking-together@2023-03-27T07:30:27.016Z</a></p>
<p><a href="https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679892669316079">futureofcoding.slack.com/archives/C5T9GPWFL/p1679892669316079</a></p>
</div> <div class="attachments"><blockquote><p>[March 26th, 2023 5:24 AM] fp: So, I'm an EUP person at heart, and this ChatGPT thing has obviously got me thinking all over again about what programming would look like to a non-technical person. At heart, I feel it should be like they're "casting spells" over reality (or virtual reality). This tips into the area of cognitive modelling: how close the physical manifestation needs to be to be able to be abstracted up to a satisfying cognitive model that matches the human's intention. In other words, you cast a spell "make that banana green!" and it comes back a lurid dayglo green, that would be a cognitive dissonance because really, you'd expect to simply get a very unripe-looking banana. What are the elements of this formalised spell-casting, this "programming system"? You have objects (banana, this one, not all ones), attributes (green, the correct one!), a sense of time or evolution (went from yellow to green). You start to get into Roget's Thesaurus land: what are the key concepts for describing the world, our human world?</p>
<p>Anyway, just a splat of the stuff buzzing around my head right now. Thoughts?</p>
<p>[March 24th, 2023 12:17 AM] wtaysom: Friends, I don't know what to make of developments in AI these days.  Having worked on dialog systems in the aughts and having loosely followed developments since (I recall preparing a talk around 2010 which left me pretty enthusiastic about ML applications in contrast to the App-and-Facebookification of "tech" — that was on time horizon of a few years, which ended up being a decade plus), every day I check in on Twitter I see more exciting stuff than I can possibly process.  I was just writing someone yesterday about how in six months time, we'll have LLMs acting as the front-end to knowledge bases and rigorous computational systems, and then we'll need to focus on getting the human, AI, and formal model all on the same page.</p>
<p>As has already been noted in &lt;#C5U3SEW6A|linking-together&gt; today, my estimate was off by roughly six months.  Consider, "I've developed a lot of plugin systems, and the OpenAI ChatGPT plugin interface might be the damn craziest and most impressive approach I've ever seen in computing in my entire life. For those who aren't aware: you write an OpenAPI manifest for your API, use human language descriptions for everything, and that's it. You let the model figure out how to auth, chain calls, process data in between, format it for viewing, etc. There's absolutely zero glue code" <a href="https://twitter.com/mitchellh/status/1638967450510458882">https://twitter.com/mitchellh/status/1638967450510458882</a>.</p>
<p>If you can tolerate his prose, Stephen Wolfram has a long post <a href="https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/">https://writings.stephenwolfram.com/2023/03/chatgpt-gets-its-wolfram-superpowers/</a>.  The "Wolfram Language as the Language for Human-AI Collaboration" section is most relevant to Future of Coding.  What do these developments mean for the Future of Coding?  And how are you all holding up?  Me?  I can hardly process what's happening, let alone what to do about it.</p>
<p>[March 27th, 2023 12:30 AM] jarno.montonen: On the heels of the "LLMs and the future of programming" discussion (<a href="https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679642239661619">https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679642239661619</a>, <a href="https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679892669316079">https://futureofcoding.slack.com/archives/C5T9GPWFL/p1679892669316079</a>), I'd like to start a more concentrated discussion around their effect on Future of Coding projects. There was already some sentiment that LLMs are going to kill FoC projects. Some yes, but certainly not all. So what kind of FoC projects LLMs will not kill?</p>
<p>[March 26th, 2023 9:51 PM] nmsmith65: Here's my perspective on LLMs and the future of programming.</p>
<p>I don't believe that the introduction of LLMs that can write code is going to obviate programming. And I don't believe that it is now pointless to develop new programming languages. Instead, I think LLMs are going to make programming and FoC research <em>better</em>, by automating one of the least interesting parts of programming: fiddling with the minutiae of syntax, language constructs, and libraries.</p>
<p>I think programmers will still have plenty of work to do. The profession is not doomed. But to justify this, we have to take a step back and consider all of the activities involved in programming.</p>
<p>Firstly, what is a "program"? A program is nothing more than:
• <em>A formal specification</em> of the behaviour of an interactive system
• ...that computer hardware can execute (after translating it into machine code).
To emphasise this, I will use the term "formal spec" in place of "program" for the remainder of this discussion.</p>
<p>GPT-4 can understand formal specs, and also everyday English. Thus, if we can describe the functionality of a system in everyday English, GPT-4 can (attempt to) translate it into a formal spec. But writing the formal spec is just <em>one</em> activity of programming.</p>
<p>Altogether, programming (or perhaps "software development") involves several activities:
1. Determining what functionality the system being developed "should" have. This is done either by talking with relevant stakeholders (e.g. the future users), or by directly observing deficiencies with their current practices.
2. Expressing that functionality as a formal specification, i.e. "coding".
3. Verifying that the specification correctly implements all of the functionality of step 1. This includes practices such as reading and reviewing the specification, as well as testing the software.
4. Validating that the implemented functionality addresses the stakeholder's problems.
5. Repeating the first 4 steps until the stakeholders are satisfied with what has been developed.
Here's my hypothesis: <em>In the next 10 years, LLMs might radically reduce the amount of work required for step 2, but <em>only</em> step 2.</em></p>
<p>Steps 1 and 4 are very human-centered, and thus can't be automated away — at least until we are at the point where we have an omnipresent AGI that observes all human practices and automatically develops solutions to improve them.</p>
<p>Similarly, step 3 will not be automated any time soon, because:
• The plain English descriptions that we give to LLMs will often be ambiguous, underspecified, and maybe even inconsistent. Thus the LLMs will have to make educated <em>guesses</em> at what we mean. (Even if they are able to ask clarifying questions, there will always be <em>some</em> choices that are automatically made for us.)
• LLMs will occasionally get confused or misinterpret what we say, even if we are clear and careful. We will not have <em>infallible</em> AIs any time soon.
So let's assume that LLMs can automate most of step 2. What does this mean for those of us developing tools and technologies to improve programming? Is our work obsolete now? Will the AI researchers and AI startups be taking the reigns?</p>
<p>I don't think so! There is still a huge opportunity to develop tools that address step 3, at the very least. (Steps 1 and 4 are harder to address with technology.)</p>
<p>In particular, <em>step 3 involves the task of <em>reading</em> source code</em>. When an LLM spits out 1000 lines of JavaScript, how do you know that the code implements the functionality that you wanted? You have to <em>verify</em> that it does, and for large programs, that will be an enormous amount of work!</p>
<p>As we all know, no amount of testing can prove that a program is correct. Thus, we cannot verify AI-generated programs just by <em>using</em> them. Maybe the program has a subtle bug, such as a buffer overflow, that might only be triggered 5 years after the program is deployed. Or less insidiously: maybe the program just doesn't handle certain edge-cases in the way you would like it to. Either way, a human should probably read through the entire program with a keen eye, to check that all of the logic <em>makes sense</em>.</p>
<p>There's clearly an opportunity for FoC researchers here: we can make languages and tools that make <em>reading</em> and <em>verifying</em> the behaviour of programs easier! Some examples:
• We can design programming languages that are vastly easier to <em>read</em> than traditional languages. How might we do that? Well, "higher-level" languages are likely easier to read, since they are likely to be more concise and focus on the end-user functionality. So work on higher-level programming models will continue to be valuable. To complement this, we can (and IMO, we should) invent new syntaxes that are closer to plain English, such that the specifications that LLMs produce are accessible to a wider audience.
• We can design programming languages where it is harder to write erroneous programs. For example, we can design programming languages that cannot crash or hang (i.e. Turing-incomplete languages), but which are still general-purpose. This reduces the kinds of errors that a human needs to consider as they verify a program.
• We can design better tools for reading and interrogating source code. (For example, better IDE support for navigating and understanding the structure of large codebases.)
• We can design better tools for exploring the space of behaviours of a running program. (Perhaps similar to the tools discussed in Bret Victor's &lt;<a href="http://worrydream.com/#!2/LadderOfAbstraction|&quot;Ladder">http://worrydream.com/#!2/LadderOfAbstraction|"Ladder</a> of Abstraction"&gt; essay.)
Overall, I think the future is bright! I'm going to continue my own PL research project (a very high-level language) with as much vigor as ever.</p>
</blockquote>
</div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T22:41:57.354Z" class="post"><span class="user">Duncan Cragg</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T22:41:57.354Z" class="date">2023-03-28 22:41:57</a> <div class="message"><p>Oh! Did I put mine first? Completely unintentional obvs.</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:36:23.428Z" class="post"><span class="user">John Krasnay</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:36:23.428Z" class="date">2023-03-28 23:36:23</a> <div class="message"><p><strong>John Krasnay</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:43:29.684Z" class="post"><span class="user">Chris Granger</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:43:29.684Z" class="date">2023-03-28 23:43:29</a> <div class="message"><p><strong>Chris Granger</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:50:20.126Z" class="post"><span class="user">Amelia Wattenberger</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:50:20.126Z" class="date">2023-03-28 23:50:20</a> <div class="message"><p><strong>Amelia Wattenberger</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:54:14.731Z" class="post"><span class="user">Katie Bell</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:54:14.731Z" class="date">2023-03-28 23:54:14</a> <div class="message"><p><strong>Katie Bell</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:56:14.451Z" class="post"><span class="user">Lu Wilson</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:56:14.451Z" class="date">2023-03-28 23:56:14</a> <div class="message"><p><strong>Lu Wilson</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-28T23:59:52.852Z" class="post"><span class="user">Denny Vrandečić</span> <a href="./?fromDate=2023-03-25&amp;toDate=2023-03-31&amp;channel=of-ai&amp;filter=#2023-03-28T23:59:52.852Z" class="date">2023-03-28 23:59:52</a> <div class="message"><p><strong>Denny Vrandečić</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T00:29:28.909Z" class="post"><span class="user">Trevor Stenson</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T00:29:28.909Z" class="date">2023-03-29 00:29:28</a> <div class="message"><p><strong>Trevor Stenson</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T00:37:27.600Z" class="post"><span class="user">Ruben Duque</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T00:37:27.600Z" class="date">2023-03-29 00:37:27</a> <div class="message"><p><strong>Ruben Duque</strong> has joined the channel</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T08:10:39.818Z" class="post"><span class="user">Jarno Montonen</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T08:10:39.818Z" class="date">2023-03-29 08:10:39</a> <div class="message"><p>ChatGPT (4) and DALL-E seem to be very slow atm, almost unusable. What if the demand for these services grows so fast that the semiconductor industry can't keep up? Could it be that AI won't take over the world within the next couple years as either there won't be enough computational power to run all the queries, or the prices will be hiked by a lot to balance the demand for the available computation power?</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T08:35:47.790Z" class="post"><span class="user">Paul Tarvydas</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T08:35:47.790Z" class="date">2023-03-29 08:35:47</a> <div class="message"><p>just goofing around...</p>
<p>pt: say a pithy comment about commenting about artificial intelligence</p>
<p>ChatGPT: Commenting on artificial intelligence is like teaching a fish to swim - we’re all still figuring it out.</p>
<p>pt: When will ChatGPT be trained only on code generated by ChatGPT?</p>
<p>ChatGPT: “...</p>
<p>pt: What is in-breeding?</p>
<p>ChatGPT:  ... it can also increase the expression of deleterious or harmful recessive alleles, leading to reduced fitness or genetic disorders. ...</p>
<p>pt: Who said “you’re fired”?</p>
<p>ChatGPT: “...</p>
<p>pt: Who said “just goofing around”?</p>
<p>ChatGPT: &lt;crickets - generating for many minutes - answer did not include ‘22 Minutes’&gt;</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T13:45:29.753Z" class="post"><span class="user">Nilesh Trivedi</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T13:45:29.753Z" class="date">2023-03-29 13:45:29</a> <div class="message"><p>Found this tool to use LLMs on command-line with pre-define roles/prompts etc: <a href="https://github.com/sigoden/aichat">github.com/sigoden/aichat</a></p>
<p>Here are some sample roles/prompts: <a href="https://github.com/olalonde/chatgpt-roles/blob/master/roles.yaml">github.com/olalonde/chatgpt-roles/blob/master/roles.yaml</a></p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-29T14:15:17.133Z" class="post"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T14:15:17.133Z" class="date">2023-03-29 14:15:17</a> <div class="message"><p><a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">futureoflife.org/open-letter/pause-giant-ai-experiments</a></p>
</div> <div class="attachments"><blockquote><p>📝 <a href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">Pause Giant AI Experiments: An Open Letter - Future of Life Institute</a></p>
</blockquote>
</div> <div class="files"></div> <div class="replies"><div id="2023-03-29T14:28:10.108Z" class="reply"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T14:28:10.108Z" class="date">2023-03-29 14:28:10</a> <div class="message"><p><a href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html">marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html</a></p>
</div> <div class="attachments"><blockquote><p>📝 <a href="https://marginalrevolution.com/marginalrevolution/2023/03/the-permanent-pause.html">The permanent pause? - Marginal REVOLUTION</a></p>
</blockquote>
</div> <div class="files"></div></div><div id="2023-03-29T15:48:04.042Z" class="reply"><span class="user">Duncan Cragg</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T15:48:04.042Z" class="date">2023-03-29 15:48:04</a> <div class="message"><p>What do you think about these, Jonathan?</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-03-29T18:06:09.777Z" class="reply"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-26&amp;toDate=2023-04-01&amp;channel=of-ai&amp;filter=#2023-03-29T18:06:09.777Z" class="date">2023-03-29 18:06:09</a> <div class="message"><p>I tend to agree with Tyler</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-03-30T16:35:03.084Z" class="reply"><span class="user">Konrad Hinsen</span> <a href="./?fromDate=2023-03-27&amp;toDate=2023-04-02&amp;channel=of-ai&amp;filter=#2023-03-30T16:35:03.084Z" class="date">2023-03-30 16:35:03</a> <div class="message"><p>It's hard to believe that none of the celebrities who signed this petition know the basics of game theory - which is all it takes to see that this appeal cannot work.</p>
</div> <div class="attachments"></div> <div class="files"></div></div></div></div><div id="2023-03-30T23:21:23.279Z" class="post"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-27&amp;toDate=2023-04-02&amp;channel=of-ai&amp;filter=#2023-03-30T23:21:23.279Z" class="date">2023-03-30 23:21:23</a> <div class="message"><p><a href="https://jimmyhmiller.github.io/">Jimmy Miller</a> what do you make of section 4 of <a href="https://www.noemamag.com/gpts-very-inhuman-mind/">noemamag.com/gpts-very-inhuman-mind</a></p>
</div> <div class="attachments"><blockquote><p>📝 <a href="https://www.noemamag.com/gpts-very-inhuman-mind/">GPT’s Very Inhuman Mind | NOEMA</a></p>
</blockquote>
</div> <div class="files"></div> <div class="replies"><div id="2023-03-31T01:04:24.638Z" class="reply"><span class="user">Srini K</span> <a href="./?fromDate=2023-03-28&amp;toDate=2023-04-03&amp;channel=of-ai&amp;filter=#2023-03-31T01:04:24.638Z" class="date">2023-03-31 01:04:24</a> <div class="message"><p>emphasis on “Artificial” from “Artificial Intelligence”</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-03-31T11:07:46.732Z" class="reply"><span class="user">Duncan Cragg</span> <a href="./?fromDate=2023-03-28&amp;toDate=2023-04-03&amp;channel=of-ai&amp;filter=#2023-03-31T11:07:46.732Z" class="date">2023-03-31 11:07:46</a> <div class="message"><p>Can't believe the internet is still arguing over whether ChatGPT is just "glorified autocomplete" 🙄</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-03-31T17:16:10.147Z" class="reply"><span class="user">Jimmy Miller</span> <a href="./?fromDate=2023-03-28&amp;toDate=2023-04-03&amp;channel=of-ai&amp;filter=#2023-03-31T17:16:10.147Z" class="date">2023-03-31 17:16:10</a> <div class="message"><p>It's a well written article. But there isn't much of an argument there. Can you define knowledge, understanding, intention, etc behavioristically? Well the article seems to assert you can, but why think that is the case?</p>
<p>The evidence we seem to be given in the article is two fold 1) Look at ChatGPT doing all these things, how can you deny it understands? 2) We can take the intentional stance towards the system and it works remarkably well.</p>
<p>I'm going to assume the article doesn't believe its presenting an argument, because if so, its a rather lackluster one. Philosophers have already talked about systems like ChatGPT well before they existed. Searle's Chinese room argument asks the exact question being raised, can a computer program be behavioristically identical to a human while not understanding?</p>
<p>Various people fall on various sides of this argument. But I don't really think ChatGPT has really any bearing on it. Of course that doesn't stop philosophers like David Chalmers from embarrassing themselves by making silly statements.</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-04-01T02:10:15.475Z" class="reply"><span class="user">Joshua Horowitz</span> <a href="./?fromDate=2023-03-29&amp;toDate=2023-04-04&amp;channel=of-ai&amp;filter=#2023-04-01T02:10:15.475Z" class="date">2023-04-01 02:10:15</a> <div class="message"><p>I found the article (or section 4 at least) quite useful. Not because it made an argument about what understanding etc. are in a philosophical sense (I sort of care about that stuff, but not really?), but because it helped me make sense of deflationary sleights of hand in the discourse.</p>
<p>When Bender et al. call GPT a “stochastic parrot” or Chiang calls it a “blurry JPEG of the web”, they are  <em><em>not</em></em>  making a Chinese-room argument about what understanding really is. They are making an argument about what GPT is  <em><em>capable of</em></em> , based on its make-up, an argument which seems to be demonstrably flawed. The “deepity” concept helped me tidy up this dynamic. As far as I’m concerned, the rest of the article may be making a misstep by making it sound like this is about deep questions of philosophy.</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-04-01T02:31:22.013Z" class="reply"><span class="user">Jimmy Miller</span> <a href="./?fromDate=2023-03-29&amp;toDate=2023-04-04&amp;channel=of-ai&amp;filter=#2023-04-01T02:31:22.013Z" class="date">2023-04-01 02:31:22</a> <div class="message"><p>I need to spend more time reading the Stochastic Parrot paper, but from my reading they are not at questioning the output of GPT style models. They seem to think they are very capable of outputting fluent speech but lack things necessary for understanding.</p>
<blockquote><p>Text generated by an LM is not grounded in communicative intent, any model of the world, or any model of the reader's state of mind. It can't have been, because the training data never included sharing thoughts with a listener, nor does the machine have the ability to do that. This can seem counter-intuitive given the increasingly fluent qualities of automatically generated text, but we have to account for the fact that our perception of natural language text, regardless of how it was generated, is mediated by our own linguistic competence and our predisposition to interpret communicative acts as conveying coherent meaning and intent, whether or not they do [89, 140]. The problem is, if one side of the communication does not have meaning, then the comprehension of the implicit meaning is an illusion arising from our singular human understanding of language (independent of the model).</p>
</blockquote>
<p>I'm not saying I necessarily agree with everything here. Just that this does seem to be the argument they are making. The deepity point was a bit off imo. From the article:</p>
<blockquote><p>The other meaning is the one that suggests that ChatGPT has no understanding of communicative intent, so when you ask it a question, it can only respond correctly in limited cases where it has seen the question, or else give awkward ill-fitting answers. But in this sense, ChatGPT is obviously not a stochastic parrot. You can ask it all sorts of subtle things, questions it has never seen before and which cannot be answered without understanding.</p>
</blockquote>
<p>Then there are some transcriptions demonstrating understanding of "communicative intent". But that's to misunderstand the point the paper was making, not to refute it. Of course GPT can talk about communicative intent. The point being made in the paper is that the ability to talk about communicative intent doesn't entail that it has communicative intent or pays attention to our communicative intent.</p>
</div> <div class="attachments"></div> <div class="files"></div></div><div id="2023-04-03T08:51:19.641Z" class="reply"><span class="user">William Taysom</span> <a href="./?fromDate=2023-03-31&amp;toDate=2023-04-06&amp;channel=of-ai&amp;filter=#2023-04-03T08:51:19.641Z" class="date">2023-04-03 08:51:19</a> <div class="message"><p>Hmm...</p>
<blockquote><p>[Text generated by an LM] can't have been [grounded in communicative intent], because the training data never included sharing thoughts with a listener</p>
</blockquote>
<p>Does this ring at all true to you guys?  I think of the "the training data" as being things people have written online, which almost always involves "sharing thoughts with a listener."</p>
<p>I take the fact that ChatGPT gives sensible answers in a way that previous GPTs did not as evidence that the model can at least preserve intent and perhaps recombine so as to generate new intent.</p>
</div> <div class="attachments"></div> <div class="files"></div></div></div></div><div id="2023-03-30T23:22:04.340Z" class="post"><span class="user">Jonathan Edwards</span> <a href="./?fromDate=2023-03-27&amp;toDate=2023-04-02&amp;channel=of-ai&amp;filter=#2023-03-30T23:22:04.340Z" class="date">2023-03-30 23:22:04</a> <div class="message"><p>Invokes Ryle and Dennett</p>
</div> <div class="attachments"></div> <div class="files"></div> <div class="replies"></div></div><div id="2023-03-31T01:27:25.316Z" class="post"><span class="user">Gustavo Goretkin</span> <a href="./?fromDate=2023-03-28&amp;toDate=2023-04-03&amp;channel=of-ai&amp;filter=#2023-03-31T01:27:25.316Z" class="date">2023-03-31 01:27:25</a> <div class="message"><p>Example of using LLM and classical verifier (a compiler for some DSL) to boost an LLM's code generation:</p>
<p><a href="https://arxiv.org/pdf/2303.14100.pdf">arxiv.org/pdf/2303.14100.pdf</a></p>
<p><a href="https://www.youtube.com/watch?v=-87yrXytluw">youtube.com/watch?v=-87yrXytluw</a></p>
</div> <div class="attachments"><blockquote><p>🎥 <a href="https://www.youtube.com/watch?v=-87yrXytluw">CLAIRify: Instruction Guided Task Programming with Verifier-Assisted Iterative Prompting</a></p>
</blockquote>
</div> <div class="files"></div> <div class="replies"></div></div>
    </div>
  </body>
</html>
